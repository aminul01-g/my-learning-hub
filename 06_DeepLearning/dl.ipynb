{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d78fa9",
   "metadata": {},
   "source": [
    "## **🤖 Phase 3: Deep Learning with PyTorch**\n",
    "We’ll start by building a neural network that recognizes handwritten digits using the classic MNIST dataset.\n",
    "\n",
    "This will teach you:\n",
    "\n",
    "* How neural networks work\n",
    "\n",
    "* How to use PyTorch (a powerful deep learning library)\n",
    "\n",
    "* Training, loss, and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76e9a2",
   "metadata": {},
   "source": [
    "#### **✅ Step 1: Install PyTorch (if not already)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b06c57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/aminul/anaconda3/envs/butterfly/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/aminul/anaconda3/envs/butterfly/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /home/aminul/anaconda3/envs/butterfly/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/aminul/anaconda3/envs/butterfly/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /home/aminul/anaconda3/envs/butterfly/lib/python3.12/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/aminul/anaconda3/envs/butterfly/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/aminul/anaconda3/envs/butterfly/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.0/865.0 MB\u001b[0m \u001b[31m495.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:38\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m548.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:19\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m556.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m580.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m603.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m575.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:25\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m591.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:09\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m596.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m490.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m596.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m587.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m503.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:09\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m553.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:11\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m595.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m581.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.22.0-cp312-cp312-manylinux_2_28_x86_64.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m578.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m579.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m661.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 mpmath-1.3.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b15a32",
   "metadata": {},
   "source": [
    "### ✅ Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10354a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68baef2d",
   "metadata": {},
   "source": [
    "1. import torch\n",
    "    * This imports the core PyTorch library.\n",
    "\n",
    "    * Think of torch like the brain of PyTorch — it handles tensors (like NumPy arrays, but more powerful) and all the basic computations.\n",
    "2. import torchvision\n",
    "\n",
    "   is a package that provides:\n",
    "      * Popular datasets (like MNIST, CIFAR-10)\n",
    "\n",
    "      * Common image models (like ResNet, AlexNet)\n",
    "\n",
    "      * Useful tools for working with images\n",
    "3. import torchvision.transforms as transforms\n",
    "   \n",
    "   helps you preprocess and augment image data, such as:\n",
    "\n",
    "      * Converting images to tensors\n",
    "\n",
    "      * Normalizing them\n",
    "\n",
    "      * Randomly flipping, cropping, resizing, etc.\n",
    "4. import torch.nn as nn\n",
    "\n",
    "   This is the Neural Network module — it includes layers like:\n",
    "\n",
    "      * nn.Linear for fully connected layers\n",
    "\n",
    "      * nn.Conv2d for convolutional layers\n",
    "\n",
    "      * nn.ReLU, nn.Softmax, etc. for activation functions\n",
    "\n",
    "   You’ll use nn to build your model architecture.\n",
    "\n",
    "5. import torch.optim as optim\n",
    "\n",
    "   This gives you optimizers like:\n",
    "\n",
    "      * optim.SGD (Stochastic Gradient Descent)\n",
    "\n",
    "      * optim.Adam (adaptive learning)\n",
    "\n",
    "   These help your model learn faster and better by updating weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df88292",
   "metadata": {},
   "source": [
    "### ✅ Step 3: Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cffc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:09<00:00, 1.01MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 91.6kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:02<00:00, 612kB/s] \n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 19.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Transform to convert images to tensor and normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Download training and test datasets\n",
    "\n",
    "# 1. Download & Transform the Training Set\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data', # folder to save the data\n",
    "    train=True,    # this is the training set\n",
    "    download=True, # download it if not already present\n",
    "    transform=transform  # apply the transform you defined earlier\n",
    ")\n",
    "\n",
    "#2. Load Training Set in Batches\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,        # the dataset to load\n",
    "    batch_size=64,   # number of images in each batch\n",
    "    shuffle=True     # shuffle data for training randomness\n",
    ")\n",
    "\n",
    "# 3.Download & Transform the Test Set\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True, transform=transform\n",
    ")\n",
    "\n",
    "# 4. Load Test Set in Batches\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33d4923",
   "metadata": {},
   "source": [
    "### 🪄 Step-by-Step:\n",
    "1. transforms.Compose([...])\n",
    "\n",
    "    Think of this like a pipeline or a to-do list.\n",
    "\n",
    "    It will apply each step in order to every image you load.\n",
    "\n",
    "2. transforms.ToTensor()\n",
    "\n",
    "    Converts the image from a PIL image (or NumPy array) into a PyTorch tensor.\n",
    "\n",
    "    Also scales pixel values from [0, 255] to [0.0, 1.0].\n",
    "\n",
    "3. transforms.Normalize((0.5,), (0.5,))\n",
    "\n",
    "    This step normalizes the image values so they are between -1 and 1.\n",
    "\n",
    "    Here's how it works for each pixel value:\n",
    "\n",
    "    new_value = (input - mean)/ std\n",
    "    \n",
    "    new_value = (original − 0.5) / 0.5\n",
    "\n",
    "    So:\n",
    "\n",
    "    * 0 → -1\n",
    "\n",
    "    * 0.5 → 0\n",
    "\n",
    "    * 1 → 1\n",
    "\n",
    "🧠 This helps the neural network learn faster and more accurately.\n",
    "### 🔎 Why (0.5,)?\n",
    "\n",
    "   * The (0.5,) is for grayscale images (like MNIST), which have only 1 channel.\n",
    "\n",
    "   * For color images (RGB), you'd use 3 values like (0.5, 0.5, 0.5).\n",
    "\n",
    "## 🧠 What’s MNIST?\n",
    "\n",
    "   * A dataset of 70,000 handwritten digits (0 through 9)\n",
    "\n",
    "   * Each image is 28×28 pixels, grayscale\n",
    "\n",
    "   * A classic dataset for learning image classification\n",
    "\n",
    "### 🟩 Download training and test datasets\n",
    "1. Download & Transform the Training Set:\n",
    "\n",
    "       trainset = torchvision.datasets.MNIST(\n",
    "       root='./data', # folder to save the data\n",
    "       train=True,    # this is the training set\n",
    "       download=True, # download it if not already present\n",
    "       transform=transform  # apply the transform you defined earlier\n",
    "       )\n",
    "   * Downloads 60,000 training images.\n",
    "\n",
    "   * Applies your transform to each image:\n",
    "\n",
    "   * Converts to tensor\n",
    "\n",
    "   * Normalizes pixel values\n",
    "\n",
    "2. Load Training Set in Batches:\n",
    "\n",
    "       trainloader = torch.utils.data.DataLoader(\n",
    "       trainset,        # the dataset to load\n",
    "       batch_size=64,   # number of images in each batch\n",
    "       shuffle=True     # shuffle data for training randomness\n",
    "       )\n",
    "\n",
    "   * Breaks data into mini-batches of 64 images each.\n",
    "\n",
    "   * shuffle=True, so that the model doesn’t see data in the same order every time.\n",
    "\n",
    "3. Download & Transform the Test Set:\n",
    "\n",
    "       testset = torchvision.datasets.MNIST(\n",
    "       root='./data',\n",
    "       train=False,\n",
    "       download=True, transform=transform\n",
    "       )\n",
    "   * This time: train=False → loads 10,000 test images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c06ee3",
   "metadata": {},
   "source": [
    "### ✅ Step 4: Define a Simple Neural Network\n",
    "\n",
    "defining a simple feedforward neural network (also called a fully connected or dense neural network) for image classification, likely on MNIST (28×28 grayscale digit images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ea672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # flatten the image\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb10233",
   "metadata": {},
   "source": [
    "1. class Net(nn.Module): \n",
    "\n",
    "   * You’re creating your own neural network class that inherits from PyTorch’s nn.Module (which gives you access to lots of helpful features).\n",
    "2. def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    * __init__ is the constructor: it sets up the layers.\n",
    "\n",
    "    * super() lets your class inherit all the cool PyTorch behaviors from nn.Module.\n",
    "3. self.fc1 = nn.Linear(28*28, 128)\n",
    "\n",
    "   * First fully connected (dense) layer\n",
    "\n",
    "   * Input: 28×28 = 784 (flattened image)\n",
    "\n",
    "   * Output: 128 neurons\n",
    "4. self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "   * Second layer: reduces 128 to 64 neurons\n",
    "5. self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "   * Final layer: maps to 10 output classes (for digits 0–9)\n",
    "6. def forward(self, x):\n",
    "    x = x.view(-1, 28*28)\n",
    "\n",
    "   * Reshapes the input image from [batch, 1, 28, 28] to [batch, 784]\n",
    "\n",
    "   * -1 means “keep the batch size the same”\n",
    "7. x = torch.relu(self.fc1(x))\n",
    "   x = torch.relu(self.fc2(x))\n",
    "\n",
    "   * Applies ReLU activation after each layer\n",
    "\n",
    "   * ReLU = Rectified Linear Unit, introduces non-linearity\n",
    "8. x = self.fc3(x)\n",
    "\n",
    "   * Final layer gives raw scores (called logits) for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5ffd4",
   "metadata": {},
   "source": [
    "### ✅ Step 5: Define Loss Function and Optimizer\n",
    "\n",
    "setting up the loss function and optimizer — perfect next step for training your neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9f2a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37113a",
   "metadata": {},
   "source": [
    "1. criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "**🔥 What it does:**\n",
    "\n",
    "   * Measures how wrong your model's predictions are.\n",
    "\n",
    "   * Perfect for classification tasks like digit recognition (0–9).\n",
    "\n",
    "**✅ Why CrossEntropyLoss?**\n",
    "\n",
    "Because:\n",
    "\n",
    "* Your model outputs 10 scores (one for each class).\n",
    "\n",
    "* CrossEntropyLoss combines:\n",
    "\n",
    "    * Softmax: turns scores into probabilities.\n",
    "\n",
    "    * Log loss: penalizes incorrect predictions more.\n",
    "\n",
    "2. optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "**🔥 What it does:**\n",
    "\n",
    " * Updates the model's weights during training to reduce the loss.\n",
    "\n",
    "***✅ Why Adam?**\n",
    "\n",
    " * It’s one of the most popular optimizers.\n",
    "\n",
    " *  Combines the best of SGD and RMSProp.\n",
    "\n",
    " * Learns fast and adaptively — great for beginners.\n",
    "\n",
    "net.parameters():\n",
    "\n",
    "  * Gives the optimizer access to all the learnable weights in your model.\n",
    "\n",
    "lr=0.001 (learning rate):\n",
    "\n",
    "   * Controls how big the steps are when updating weights.\n",
    "\n",
    "   * 0.001 is a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755f25f2",
   "metadata": {},
   "source": [
    "### ✅ Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f42a87b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 374.266\n",
      "Epoch 2, Loss: 184.975\n",
      "Epoch 3, Loss: 133.082\n",
      "Epoch 4, Loss: 105.000\n",
      "Epoch 5, Loss: 89.613\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # 5 epochs\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24988a26",
   "metadata": {},
   "source": [
    "1. for epoch in range(5):\n",
    "\n",
    "   * You're training the model for 5 full passes through the training data (called \"epochs\").\n",
    "\n",
    "2. for images, labels in trainloader:\n",
    "\n",
    "   *  trainloader gives you batches of data (e.g., 64 images + labels at a time).\n",
    "\n",
    "   * images: input data (like pictures of digits).\n",
    "\n",
    "   * labels: the correct answers (e.g., 7, 2, 0, etc.).\n",
    "\n",
    "3. optimizer.zero_grad()\n",
    "\n",
    "   * Clears out any old gradients from the previous batch (PyTorch accumulates them by default).\n",
    "\n",
    "4. outputs = net(images)\n",
    "\n",
    "   * Runs the images through your model and gets predictions (raw scores).\n",
    "\n",
    "5. loss = criterion(outputs, labels)\n",
    "\n",
    "   * Compares predictions to the correct labels.\n",
    "\n",
    "   * Computes how wrong the model is (the \"loss\").\n",
    "\n",
    "6. loss.backward()\n",
    "\n",
    "   * Calculates gradients (how much each weight should change).\n",
    "\n",
    "7. optimizer.step()\n",
    "\n",
    "   * Updates the model weights using those gradients.\n",
    "\n",
    "8. running_loss += loss.item()\n",
    "\n",
    "   * Adds up the loss from each batch to track total loss for the epoch.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb1b26",
   "metadata": {},
   "source": [
    "### ✅ Step 7: Evaluate Accuracy\n",
    "\n",
    "Check how well your model performs on the test dataset (data it has never seen before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af29e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.67%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff52732",
   "metadata": {},
   "source": [
    "1. with torch.no_grad():\n",
    "\n",
    "   * This tells PyTorch not to calculate gradients (which saves memory and speeds things up).\n",
    "\n",
    "   * You don't need gradients during testing — only during training.\n",
    "\n",
    "2. for images, labels in testloader:\n",
    "\n",
    "   * You're getting batches of images + their correct labels from the testloader.\n",
    "\n",
    "4. outputs = net(images)\n",
    "\n",
    "   * Make Predictions\n",
    "   * Run the images through the model to get output scores for each class.\n",
    "\n",
    "5. _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "   * outputs.data contains the scores for each class (10 numbers per image).\n",
    "\n",
    "   * torch.max(..., 1) gets the index of the highest score, i.e., the predicted class (e.g., \"3\" or \"7\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1416f3f2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "butterfly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
