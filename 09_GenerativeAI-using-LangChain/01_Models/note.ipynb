{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7be547",
   "metadata": {},
   "source": [
    "## üìù LangChain Models Overview\n",
    "\n",
    "LangChain provides wrappers around different kinds of models so you can plug them into pipelines (chains, agents, tools) easily. The three big categories are:\n",
    "\n",
    "### üîπ 1. LLMs (Large Language Models)\n",
    "‚úÖ What:\n",
    "\n",
    " - Core text-generation models (GPT, Falcon, LLaMA, etc.).\n",
    "\n",
    " - Given a prompt, they return a completion (string).\n",
    "\n",
    " - Example: HuggingFaceEndpoint, OpenAI in LangChain.\n",
    "\n",
    "‚úÖ Why:\n",
    "\n",
    " - They form the foundation for reasoning, writing, and general-purpose NLP tasks.\n",
    "\n",
    " - Good for summarization, Q&A, content generation, code writing.\n",
    "\n",
    "‚úÖ When:\n",
    "\n",
    " - Use when you need raw completions from models.\n",
    "\n",
    " - Best for single-turn tasks (input ‚Üí output).\n",
    "\n",
    "‚úÖ How:\n",
    "\n",
    "```bash\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "llm = HuggingFaceEndpoint(repo_id=\"tiiuae/falcon-7b-instruct\", task=\"text-generation\")\n",
    "result = llm.invoke(\"Explain AI in simple words.\")\n",
    "\n",
    "```\n",
    "\n",
    "### üîπ 2. Chat Models\n",
    "‚úÖ What:\n",
    "\n",
    " - Specialized wrappers around models that understand conversation format (messages with roles: system, user, assistant).\n",
    "\n",
    " - Example: ChatOpenAI, ChatHuggingFace.\n",
    "\n",
    "‚úÖ Why:\n",
    "\n",
    " - Makes it easier to build multi-turn chatbots and maintain context.\n",
    "\n",
    " - Supports structured inputs instead of plain text.\n",
    "\n",
    "‚úÖ When:\n",
    "\n",
    " - Use for conversational agents, assistants, or apps where context + role matters.\n",
    "\n",
    "‚úÖ How:\n",
    "\n",
    "```bash\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", task=\"text-generation\")\n",
    "chat_model = ChatHuggingFace(llm=llm)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is the capital of Bangladesh?\")\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)  # \"The capital of Bangladesh is Dhaka.\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### üîπ 3. Embedding Models\n",
    "‚úÖ What:\n",
    "\n",
    "Models that convert text into vector embeddings (numerical representation of meaning).\n",
    "\n",
    "Example: HuggingFaceEmbeddings, OpenAIEmbeddings.\n",
    "\n",
    "‚úÖ Why:\n",
    "\n",
    "Used for semantic similarity, search, retrieval, and clustering.\n",
    "\n",
    "Forms the backbone of RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "‚úÖ When:\n",
    "\n",
    "Use when you want to compare meanings of texts, store docs in a vector database, or build search/retrieval systems.\n",
    "\n",
    "‚úÖ How:\n",
    "\n",
    "```bash\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector = embeddings.embed_query(\"What is quantum computing?\")\n",
    "print(len(vector))  # embedding dimension\n",
    "\n",
    "```\n",
    "\n",
    "### üëâ Think of it like this:\n",
    "\n",
    "- LLMs = Brain that generates text.\n",
    "\n",
    "- Chat Models = Same brain but trained to have conversations.\n",
    "\n",
    "- Embeddings = Brain‚Äôs way of \"understanding\" meaning mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b5b14",
   "metadata": {},
   "source": [
    "## Hugging Face models can be run in two different ways:\n",
    "\n",
    "\n",
    "1. Using Hugging Face Inference API (hosted by Hugging Face)\n",
    "\n",
    "Instead of downloading the weights, you call Hugging Face‚Äôs hosted API.\n",
    "\n",
    "The model runs on Hugging Face‚Äôs servers (not on your machine).\n",
    "\n",
    "You just send your input (prompt) over the internet and get back the output.\n",
    "\n",
    "No need for a powerful GPU or extra libraries.\n",
    "\n",
    "Example using huggingface_hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b668856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\"tiiuae/falcon-7b-instruct\")\n",
    "\n",
    "response = client.text_generation(\"Explain quantum computing simply:\", max_new_tokens=200)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e6c942",
   "metadata": {},
   "source": [
    "2. Running a model locally\n",
    "\n",
    "You download the model weights (the .bin / .safetensors files) from Hugging Face Hub into your computer.\n",
    "\n",
    "Then you use transformers (with AutoModelForCausalLM, AutoTokenizer, etc.) to load and run the model directly on your hardware (CPU or GPU).\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cbe1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# move model to GPU manually\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "inputs = tokenizer(\"Explain quantum computing simply:\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
